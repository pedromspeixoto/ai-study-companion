# Authentication
# Generate a random secret: `openssl rand -base64 32`
AUTH_SECRET=change-me

# Database (matches docker-compose defaults)
POSTGRES_URL=postgres://ai_rag:ai_rag@localhost:5432/ai_rag

# Local file uploads (defaults to ./public/uploads when unset)
# LOCAL_UPLOAD_DIR=./public/uploads

# =============================================================================
# Model Provider Configuration
# =============================================================================

# Ollama Configuration (DEFAULT - recommended for local development)
# Ollama is used by default for chat models, titles, and embeddings
# If OLLAMA_BASE_URL is set, Ollama will be used instead of OpenAI/Anthropic
# 
# IMPORTANT: Ollama runs locally (not in Docker) - run: ./scripts/setup-ollama-macos.sh
# 
# For local development: http://localhost:11434 (default)
# For Docker containers: http://host.docker.internal:11434
OLLAMA_BASE_URL=http://localhost:11434

# OpenAI API Key (OPTIONAL - only needed if not using Ollama)
# Used for: embeddings (must match Dagster pipeline) + OpenAI chat models
# Only required if OLLAMA_BASE_URL is not set
OPENAI_API_KEY=change-me

# Anthropic API Key (OPTIONAL - only needed if using Claude models)
# Used for: Claude chat models only
ANTHROPIC_API_KEY=change-me

# =============================================================================
# Title Generation
# =============================================================================

# Which provider to use for generating chat titles
# Options: openai | anthropic | ollama (default when OLLAMA_BASE_URL is set)
# Defaults to ollama if OLLAMA_BASE_URL is set, otherwise openai
TITLE_PROVIDER=ollama

# =============================================================================
# Optional Services
# =============================================================================

# Redis URL for resumable streams (docker compose exposes this on localhost:6379)
REDIS_URL=redis://localhost:6379

# AI SDK Variables
AI_SDK_LOG_WARNINGS=
