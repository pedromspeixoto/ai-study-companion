# =============================================================================
# Ollama Configuration (DEFAULT - recommended for local development)
# =============================================================================

# Ollama is used by default for chat models, titles, and embeddings
# If OLLAMA_BASE_URL is set, Ollama will be used instead of OpenAI/Anthropic
# 
# IMPORTANT: Ollama runs locally (not in Docker) - run: ./scripts/setup-ollama-macos.sh
# 
# For Docker containers: http://host.docker.internal:11434 (default)
# For local development: http://localhost:11434
OLLAMA_BASE_URL=http://host.docker.internal:11434

# =============================================================================
# API Keys (OPTIONAL - only needed if not using Ollama)
# =============================================================================

# OpenAI API Key (OPTIONAL - only needed if OLLAMA_BASE_URL is not set)
# Used for: embeddings (data pipeline) + OpenAI chat models
# Get your key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your-openai-api-key-here

# Anthropic API Key (OPTIONAL)
# Used for: Claude chat models only
# Get your key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here

# =============================================================================
# Title Generation
# =============================================================================

# Which provider to use for generating chat titles
# Options: openai | anthropic | ollama (default when OLLAMA_BASE_URL is set)
# Defaults to ollama if OLLAMA_BASE_URL is set, otherwise openai
TITLE_PROVIDER=ollama

# =============================================================================
# Authentication
# =============================================================================

# Auth secret for frontend (generate with: openssl rand -base64 32)
AUTH_SECRET=change-me
