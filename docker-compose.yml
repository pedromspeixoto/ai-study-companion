services:
  postgres:
    image: pgvector/pgvector:pg16
    container_name: study-companion-postgres
    environment:
      POSTGRES_DB: ai_rag
      POSTGRES_USER: ai_rag
      POSTGRES_PASSWORD: ai_rag
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ai_rag -d ai_rag"]
      interval: 5s
      timeout: 5s
      retries: 10
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - ai-stack
    restart: unless-stopped

  redis:
    image: redis:7.4-alpine
    container_name: study-companion-redis
    command: ["redis-server", "--save", "60", "1", "--loglevel", "warning"]
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 10
    volumes:
      - redis-data:/data
    networks:
      - ai-stack
    restart: unless-stopped

  pgweb:
    image: sosedoff/pgweb:latest
    container_name: study-companion-pgweb
    ports:
      - "8081:8081"
    environment:
      PGWEB_DATABASE_URL: postgres://ai_rag:ai_rag@postgres:5432/ai_rag?sslmode=disable
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - ai-stack
    restart: unless-stopped

  # Ollama service - COMMENTED OUT by default since I run on MacOS
  # 
  # For macOS: Run Ollama natively for Metal GPU acceleration (recommended)
  #   Run: ./scripts/setup-ollama-macos.sh
  #   Docker containers will connect via: http://host.docker.internal:11434
  #
  # For Linux with NVIDIA GPU: Uncomment this service and use Docker GPU support
  #   Run: ./scripts/setup-docker-gpu.sh first to configure GPU support
  #   Then uncomment the ollama service below and set OLLAMA_BASE_URL=http://ollama:11434
  #
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: ollama
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama_data:/root/.ollama
  #     - ./scripts/ollama-init.sh:/usr/local/bin/ollama-init.sh:ro
  #   environment:
  #     - OLLAMA_HOST=0.0.0.0
  #     # CPU optimization: Set to cpu_avx2 (best), cpu_avx (moderate), or cpu (compatible)
  #     # Leave unset to auto-detect, or set explicitly for CPU-only systems
  #     - OLLAMA_LLM_LIBRARY=${OLLAMA_LLM_LIBRARY:-}
  #   entrypoint: []
  #   command: ["/bin/sh", "/usr/local/bin/ollama-init.sh"]
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 5
  #     start_period: 120s
  #   networks:
  #     - ai-stack
  #   restart: unless-stopped
  #   # GPU support - requires NVIDIA GPU and nvidia-container-toolkit installed
  #   # Run: ./scripts/setup-docker-gpu.sh to configure
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]

  dagster:
    build:
      context: ./data
      dockerfile: Dockerfile
    container_name: study-companion-dagster
    ports:
      - "3001:3000"
    volumes:
      # Mount features directory so PDFs are accessible
      - ./data/features:/app/features:ro
      # Mount source code for development (optional, can be removed for production)
      - ./data/src:/app/src:ro
      # Mount workspace.yaml for Dagster configuration
      - ./data/workspace.yaml:/app/workspace.yaml:ro
      # Persist Dagster home directory
      - dagster-home:/app/.dagster_home
    environment:
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=ai_rag
      - POSTGRES_USER=ai_rag
      - POSTGRES_PASSWORD=ai_rag
      - DAGSTER_HOME=/app/.dagster_home
      - PYTHONPATH=/app/src
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      # Ollama Configuration (optional - uncomment to use Ollama instead of OpenAI)
      # - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://host.docker.internal:11434}
    depends_on:
      postgres:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - ai-stack

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: study-companion-frontend
    ports:
      - "1337:3000"
    environment:
      - AUTH_SECRET=${AUTH_SECRET:-change-me}
      - AUTH_URL=http://localhost:1337
      - AUTH_TRUST_HOST=true
      - POSTGRES_URL=postgres://ai_rag:ai_rag@postgres:5432/ai_rag
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - TITLE_PROVIDER=${TITLE_PROVIDER:-openai}
      # Ollama Configuration (optional - uncomment to use Ollama instead of OpenAI)
      # - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://host.docker.internal:11434}
      # - OLLAMA_CHAT_MODEL=${OLLAMA_CHAT_MODEL:-llama3.1:8b}
      # - OLLAMA_TITLE_MODEL=${OLLAMA_TITLE_MODEL:-}
      - REDIS_URL=redis://redis:6379
      - NODE_ENV=production
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - ai-stack
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:3000', (r) => {process.exit(r.statusCode === 200 ? 0 : 1)})"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

volumes:
  postgres-data:
  redis-data:
  dagster-home:
  # ollama_data:  # Uncomment if using Ollama in Docker

networks:
  ai-stack:
